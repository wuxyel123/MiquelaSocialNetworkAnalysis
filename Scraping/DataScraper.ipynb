{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=''\n",
    "with open('.token', 'r') as f:\n",
    "    token = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#Read .token file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.ensembledata.com/apis\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#Read .token file\n",
    "\n",
    "root = \"https://www.ensembledata.com/apis\"\n",
    "endpoint = \"/tt/user/posts\"\n",
    "params = {\n",
    "  \"username\": \"lilmiquela\",\n",
    "  \"depth\": 50,\n",
    "  \"start_cursor\": 0,\n",
    "  \"oldest_createtime\": 0,\n",
    "  \"token\": token\n",
    "}\n",
    "\n",
    "\n",
    "res = requests.get(root+endpoint, params=params)\n",
    "print(res.json())\n",
    "#Save response to file\n",
    "with open('lilmiquela_posts.json', 'w') as f:\n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next fragment of code is used to collect all hashtags from the posts, that way we can select which one to use as a flag to identify 'past' and 'present' posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inlove', 'whatwillilooklike', 'summerrecap', 'conspiracytheory', 'animegirl', 'hbo', 'illblowup', 'howtosmile', 'dior', 'talkingtothemoon', 'idontwannagothere', 'moneymusicvideo', 'factsaboutme', 'breakup', 'plottwist', '10pictureschallenge', 'bored', 'notthemaincharacter', 'feliznavidad', 'glitch', 'polarexpress', 'pacsun', 'foodontiktok', 'parents', 'getrealmiquela', 'levitating', 'rosalia', 'thephoto', 'bitcoin', 'familyimpression', 'swimmercheck', 'diyprojects', 'deeprealization', 'spelling', 'nosepainting', 'fitcheck', 'naturescereal', 'gaminglife', 'society', 'questionsigetasked', 'renaissancechallenge', 'teslarobot', 'dontbesurprised', 'karpoolkaraoke', 'mentalhealthawareness', 'robotversion', 'crazy', 'qanda', 'relatable', 'squidgame', 'remix', 'letsplay', 'hack', 'storytime', 'answeringquestions', 'fy', 'realtalk', 'wellness', 'lifeisgood', 'notjustme', 'animeedit', 'checkyourvibe', 'mustbenice', 'wewintogether', 'cozyathome', 'inverted', 'good4u', 'kendricklamar', 'enchanted', 'springvibes', 'manifesting', 'duet', 'onlinedating', 'emo', 'illneverforgetyou', 'lifestyle', 'buffering', 'olympics', 'holidayaesthetic', 'runningqueen', 'indigenouspeoplesday', 'unrealisticchristmaslist', 'holidaytiktok', 'howbizarre', 'thisismysong', 'distancedance', 'wishlist', 'songassociationchallenge', 'sorry', 'iiipoints', 'golittlerockstar', 'mcqueeneyewear', '2020rewind', 'summerdiy', 'happyholidays', 'charging', 'villam', 'imsmartbutimstupid', 'oop', 'shootyourshot', 'olympicspirit', 'notaperfectperson', 'yearontiktok', 'barbz', 'onethingaboutme', 'makeyourmove', 'rightback', 'nickiminaj', 'ihavearrived', 'thatsnotmyname', 'netflix', 'upgradechallenge', 'timewarpjump', 'whenimolder', 'sciencetok', 'yabadabadoba', 'momsoftiktok', 'filter', 'dramaticmoments', 'nopeyup', '3moods', 'bts', 'whatiwouldfeed', 'taurus', 'fail', 'makeup', 'thewomanwastoostunnedtospeak', 'unlockit', 'taylorswift', 'siblinggoals', 'weekendvibes', 'holidaycountdown', 'hugoyourway', 'hairtok', 'waitforend', 'naruto', 'newsong', 'andithinktomyself', 'youandme', 'dad', 'realization', 'bestfriend', 'foryourpage', 'newmusicfriday', 'yearinreview', 'whatatimetobealive', 'fyp„Ç∑', 'transformation', 'mycalvins', 'robot', 'backintheday', 'comment', 'replyingtocomments', 'crush', 'throwback', 'capcut', 'waitforit', 'glowup', 'spongebob', 'teenagedirtbag', 'wee', 'adayinmylife', 'music', 'everydayolympics', 'streetfashion', 'squidgamenetflix', 'feelings', 'seventeen', 'driverslicense', 'dayinmylife', 'vibing', 'meexplaining', 'prague', 'replytocomments', 'monthsoftheyear', 'superspy', 'mindwanders', 'horoscope', 'rhymepov', 'cryptok', 'leftmeonread', 'vibewithme', 'arewedoingthisright', 'viral', 'oopss', 'humor', 'materialgworl', 'joselinescabaret', 'alexandermcqueen', 'casualfashion', 'evolution', 'dontspeak', 'lookalike', 'tiktokfashionmonth', 'challenge', 'justdancemoves', 'vlog', 'daynnite', 'confidence', 'wrappinggifts', 'maybewhenimdancing', 'tiktokphotography', 'welcome2021', 'vibes', 'moneymusic', 'justvibing', 'iknowsomethingyoudont', 'justinbieber', '2018vs2021', 'euphoria', 'backyardigans', 'illneverbeher', 'couplestiktok', 'euphoriaseason2', 'wtf', 'gamingvibes', 'hottake', 'fetapasta', 'freebritney', 'mermaid', 'lookbook', 'couplethings', 'crazystory', 'beforeandafter', '26', 'life', 'immuneupvapedown', 'exploring', 'mepracticing', 'youdontknow', 'smilechallenge', 'goingthroughsomething', 'anime', 'doaflip', 'kpop', 'healthyliving', 'countdown', 'ilikeyouhaveacupcake', '4', 'sailormoon', 'metaverse', 'breakuptiktok', '2021rewind', 'mumcheck', 'favorites', 'modelfacechallenge', 'fallfashion', 'ohtakemebacktothenightwemet', 'thingsimstartingtolove', 'mamasaid', 'fyp', '27videoschallenge', 'smiling', 'mylife', 'altrocktober', 'thinkingabout', 'speakup', 'realandfake', 'tiktokart', 'rating', 'realizations', 'mytruth', 'captcha', 'problemsolved', 'food', 'pose', 'mcqueenseal', 'friends', 'teamusa', 'animechallenge', 'isthisavailable', 'eyebrows', 'beentheredonethat', 'dontdoit', 'lilmiquela', 'outerbanksseason2', 'pushinüÖøÔ∏è', 'tokyoolympics', 'manifestation', 'pov', 'miquela', '16missedcalls', 'dejavu', 'ghostmode', 'mmmchallenge', 'robotgirl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "#read file as a string\n",
    "with open('lilmiquela_posts.json', 'r') as f:\n",
    "    data = f.read()\n",
    "#convert string to json\n",
    "data = json.loads(data)\n",
    "all_hashtags = []\n",
    "for post in data['data']:\n",
    "    #Get all the words appearing after #\n",
    "    hashtags = [word for word in post['desc'].split() if word[0] == '#']\n",
    "    #Remove # from hashtags\n",
    "    hashtags = [word[1:] for word in hashtags]\n",
    "    #convert to lower case\n",
    "    hashtags = [word.lower() for word in hashtags]\n",
    "    #Add hashtags to list\n",
    "    all_hashtags.append(hashtags)\n",
    "#remove all duplicates from list\n",
    "all_hashtags = list(set([item for sublist in all_hashtags for item in sublist]))\n",
    "print(all_hashtags)\n",
    "#Save hashtags to csv file\n",
    "with open('lilmiquela_hashtags.csv', 'w') as f:\n",
    "    f.write('\\n'.join(all_hashtags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get all the statistics from each post, we split them in 2 lists (one for present and one for past), and we get the id of the 3 best and 3 worst post of the two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 3 Past Posts: [['7040550342274092294', 'All Glewed Up #Robot #16missedcalls  #backintheday', 20859873, 1837914, 18422, 16625], ['7129589619099847982', 'I win this trend #teenagedirtbag #emo #throwback', 11306092, 1472456, 6497, 3362], ['6820442784093228294', 'Did I invent a thing? IDK, but it feels like it. #breakup #feelings #notjustme', 8803245, 754274, 10691, 11592]]\n",
      "Best 3 Present Posts: [['6857638022805032197', 'What‚Äôs Polar Express? #ReplyToComments #NopeYup #CheckYourVibe', 50937625, 7864894, 110033, 52887], ['6821996652581113093', 'It‚Äôs upsetting me and my homegirl #answeringquestions #fy #mepracticing', 23897721, 2956151, 50698, 48269], ['7027954993026190598', 'tell me ur favorite robots #ilikeyouhaveacupcake #rating #robot', 11180434, 1337570, 5727, 2306]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "#read file as a string\n",
    "with open('lilmiquela_posts.json', 'r') as f:\n",
    "    data = f.read()\n",
    "#convert string to json\n",
    "data = json.loads(data)\n",
    "#Get post titles\n",
    "#Def empty matrix\n",
    "posts = {}\n",
    "#Define a list of flags that define throwback posts\n",
    "past_flags= ['whenimolder', 'illneverforgetyou', 'backintheday', 'teenagedirtbag', 'waitforit', 'transformation', 'whatwillilooklike', 'notaperfectperson', 'dejavu', 'ohtakemebacktothenightwemet', 'evolution', 'breakup', 'upgradechallenge', 'yearinreview', '2018vs2021', 'mylife', 'beentheredonethat', 'storytime', '2020rewind', 'justdancemoves', 'crazystory', '2021rewind', 'timewarpjump', 'deeprealization', 'yearontiktok', 'leftmeonread', 'breakuptiktok', 'throwback', 'beforeandafter']\n",
    "past_posts = {}\n",
    "present_posts = {}\n",
    "\n",
    "# Function to calculate a simple engagement score for a post\n",
    "def calculate_engagement_score(post):\n",
    "    play_count, digg_count, comment_count, share_count = post[2], post[3], post[4], post[5]\n",
    "    # Simple formula to calculate score: Sum of all counts\n",
    "    return play_count + digg_count + comment_count + share_count\n",
    "\n",
    "# Process each post in the data\n",
    "for i, post in enumerate(data['data']):\n",
    "    # Extract relevant information from each post\n",
    "    post_info = [post['aweme_id'], post['desc'], post['statistics']['play_count'], post['statistics']['digg_count'], post['statistics']['comment_count'], post['statistics']['share_count']]\n",
    "    \n",
    "    # Extract hashtags from the post description\n",
    "    hashtags = [word[1:].lower() for word in post['desc'].split() if word.startswith('#')]\n",
    "\n",
    "    # Check if any of the hashtags matches the past flags\n",
    "    is_past_post = any(hashtag in past_flags for hashtag in hashtags)\n",
    "\n",
    "    # Categorize the post as either past or present based on hashtags\n",
    "    if is_past_post:\n",
    "        past_posts[i] = post_info\n",
    "    else:\n",
    "        present_posts[i] = post_info\n",
    "\n",
    "# Finding the best 3 posts for past and present based on engagement score\n",
    "best_past_posts = sorted(past_posts.values(), key=calculate_engagement_score, reverse=True)[:3]\n",
    "best_present_posts = sorted(present_posts.values(), key=calculate_engagement_score, reverse=True)[:3]\n",
    "\n",
    "# Output the best 3 past and present posts\n",
    "print(\"Best 3 Past Posts:\", best_past_posts)\n",
    "print(\"Best 3 Present Posts:\", best_present_posts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_all_comments(aweme_id, token, max_comments=300):\n",
    "    root = \"https://www.ensembledata.com/apis\"\n",
    "    endpoint = \"/tt/post/comments\"\n",
    "    all_comments = []\n",
    "    cursor = 0\n",
    "\n",
    "    while len(all_comments) < max_comments:\n",
    "        params = {\n",
    "            \"aweme_id\": aweme_id,\n",
    "            \"cursor\": cursor,\n",
    "            \"token\": token\n",
    "        }\n",
    "        response = requests.get(root + endpoint, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        comments = data.get('data', {}).get('comments', [])\n",
    "        all_comments.extend(comments[:max_comments - len(all_comments)])\n",
    "        \n",
    "        # Update the cursor for the next iteration\n",
    "        cursor = data.get('data', {}).get('nextCursor', 0)\n",
    "        \n",
    "        # Break if there are no more comments\n",
    "        if cursor == 0:\n",
    "            break\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# Arrays of tokens and aweme_id values\n",
    "tokens = [\"3MfzVGSgUkHF1vWr\", \"qoQANMRiT2pSYTQC\", \"9vit7ZC5TBelMgyr\"]  # Replace with your actual tokens\n",
    "best_past_posts_aweme_ids = [7040550342274092294,7129589619099847982,6820442784093228294]\n",
    "best_present_posts_aweme_ids = [6857638022805032197,6821996652581113093,7027954993026190598]\n",
    "\n",
    "# Initialize a dictionary to store comments\n",
    "all_comments = {}\n",
    "\n",
    "# Fetch and store comments for each post using different tokens\n",
    "for i, aweme_id in enumerate(best_past_posts_aweme_ids + best_present_posts_aweme_ids):\n",
    "    token = tokens[i % len(tokens)]  # Round-robin selection of token\n",
    "    comments = fetch_all_comments(aweme_id, token)\n",
    "    all_comments[aweme_id] = comments\n",
    "\n",
    "# Save the comments to a file\n",
    "with open('lilmiquela_all_comments.json', 'w') as f:\n",
    "    json.dump(all_comments, f, indent=4)\n",
    "\n",
    "print(\"Comments fetched and saved for all best posts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide comments in present and past and keep only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_past_posts_aweme_ids = [7040550342274092294,7129589619099847982,6820442784093228294]\n",
    "best_present_posts_aweme_ids = [6857638022805032197,6821996652581113093,7027954993026190598]\n",
    "#Load all comments from file\n",
    "with open('lilmiquela_all_comments.json', 'r') as f:\n",
    "    all_comments = json.load(f)\n",
    "#Get all comments for each past post\n",
    "past_post_comments = []\n",
    "comments_tmp = []\n",
    "for id in best_past_posts_aweme_ids:\n",
    "    comments_tmp = all_comments[str(id)]\n",
    "    for comment in comments_tmp:\n",
    "        past_post_comments.append(comment['text'])\n",
    "#Get all comments for each present post\n",
    "present_post_comments = []\n",
    "for id in best_present_posts_aweme_ids:\n",
    "    comments_tmp = all_comments[str(id)]\n",
    "    for comment in comments_tmp:\n",
    "        present_post_comments.append(comment['text'])\n",
    "#Save comments to csv file  \n",
    "with open('lilmiquela_past_comments.csv', 'w') as f:\n",
    "    f.write('\\n'.join(past_post_comments))\n",
    "with open('lilmiquela_present_comments.csv', 'w') as f:\n",
    "    f.write('\\n'.join(present_post_comments))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis\n",
    "Sentiment analysis pipeline\n",
    "https://huggingface.co/finiteautomata\n",
    "https://huggingface.co/blog/sentiment-analysis-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all comments from file\n",
    "#Past comments\n",
    "with open('lilmiquela_past_comments.csv', 'r') as f:\n",
    "    past_comments = f.read().splitlines()\n",
    "#Present comments\n",
    "with open('lilmiquela_present_comments.csv', 'r') as f:\n",
    "    present_comments = f.read().splitlines()\n",
    "\n",
    "#Remove all commas from comments\n",
    "past_comments = [comment.replace(',', '') for comment in past_comments]\n",
    "present_comments = [comment.replace(',', '') for comment in present_comments]\n",
    "    \n",
    "from transformers import pipeline\n",
    "specific_model_emotion = pipeline(model=\"finiteautomata/bertweet-base-emotion-analysis\")\n",
    "specific_model_sentiment = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "\n",
    "#Get emotion and sentiment for past comments\n",
    "keywords_past = kw_model.extract_keywords(past_comments,keyphrase_ngram_range=(1, 1),stop_words='english',highlight=False,top_n=1)\n",
    "emotion_res_past = specific_model_emotion(past_comments)\n",
    "sentiment_res_past = specific_model_sentiment(past_comments)\n",
    "#Get emotion and sentiment for present comments\n",
    "keywords_present = kw_model.extract_keywords(present_comments,keyphrase_ngram_range=(1, 1),stop_words='english',highlight=False,top_n=1)\n",
    "emotion_res_present = specific_model_emotion(present_comments)\n",
    "sentiment_res_present = specific_model_sentiment(present_comments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=['comment','keyword','emotion', 'sentiment']\n",
    "#Save emotion and sentiment to csv file in the format: comment,keyword, emotion, sentiment\n",
    "with open('sentiment_analysis/lilmiquela_past_emotion_sentiment.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    for i in range(len(past_comments)):\n",
    "        keyword = keywords_past[i]\n",
    "        #If not keyword found, use the comment as keyword\n",
    "        if len(keyword) == 0:\n",
    "            keyword = past_comments[i]\n",
    "        else:\n",
    "            keyword = keyword[0][0]\n",
    "\n",
    "        f.write(past_comments[i]+','+keyword+','+emotion_res_past[i]['label']+','+sentiment_res_past[i]['label']+'\\n')\n",
    "with open('sentiment_analysis/lilmiquela_present_emotion_sentiment.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    for i in range(len(present_comments)):\n",
    "        keyword = keywords_present[i]\n",
    "        #If not keyword found, use the comment as keyword\n",
    "        if len(keyword) == 0:\n",
    "            keyword = present_comments[i]\n",
    "        else:\n",
    "            keyword = keyword[0][0]\n",
    "\n",
    "        f.write(present_comments[i]+','+keyword+','+emotion_res_present[i]['label']+','+sentiment_res_present[i]['label']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read emotion and sentiment from csv file\n",
    "import pandas as pd\n",
    "past_df = pd.read_csv('sentiment_analysis/lilmiquela_past_emotion_sentiment.csv')\n",
    "present_df = pd.read_csv('sentiment_analysis/lilmiquela_present_emotion_sentiment.csv')\n",
    "\n",
    "#Count the number of times each keyword is associated with each emotion and save for each keyword the emotion with the highest count\n",
    "past_dict = {}\n",
    "for index, row in past_df.iterrows():\n",
    "    keyword = row['keyword']\n",
    "    emotion = row['emotion']\n",
    "    if keyword not in past_dict:\n",
    "        past_dict[keyword] = {}\n",
    "    if emotion not in past_dict[keyword]:\n",
    "        past_dict[keyword][emotion] = 0\n",
    "    past_dict[keyword][emotion] += 1\n",
    "\n",
    "#Create a csv file with the format: id,label\n",
    "header=['Id','Label']\n",
    "emotion_ids = []\n",
    "keyword_ids = []\n",
    "rows = []\n",
    "i=1\n",
    "for emotion in past_df['emotion'].value_counts().index.tolist():\n",
    "    rows.append(str(i)+\",\"+emotion)\n",
    "    emotion_ids.append(emotion)\n",
    "    i+=1\n",
    "for keyword in past_dict:\n",
    "    rows.append(str(i)+\",\"+keyword)\n",
    "    keyword_ids.append(keyword)\n",
    "    i+=1\n",
    "\n",
    "#Save rows to csv file\n",
    "with open('sentiment_analysis/gephi/lilmiquela_past_emotion_nodes.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    f.write('\\n'.join(rows))\n",
    "\n",
    "#Create a csv file with the format: Source,Target,Label,Weight\n",
    "header=['Source','Target','Label','Weight']\n",
    "rows = []\n",
    "i=1\n",
    "for keyword,emotion in past_dict.items():\n",
    "    #Get emotion key from emotion value\n",
    "    emotion_key = list(emotion.keys())[0]\n",
    "    #Get emotion count as weight\n",
    "    weight = list(emotion.values())[0]\n",
    "    #Get emotion id\n",
    "    target = emotion_ids.index(emotion_key)+1\n",
    "    #Get keyword id\n",
    "    source = i+len(emotion_ids)\n",
    "    rows.append(str(source)+\",\"+str(target)+\",\"+keyword+\" - \"+emotion_key+\",\"+str(weight))\n",
    "    i+=1\n",
    "#Save rows to csv file\n",
    "with open('sentiment_analysis/gephi/lilmiquela_past_emotion_edges.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    f.write('\\n'.join(rows))\n",
    "\n",
    "#Same for present comments\n",
    "present_dict = {}\n",
    "for index, row in present_df.iterrows():\n",
    "    keyword = row['keyword']\n",
    "    emotion = row['emotion']\n",
    "    if keyword not in present_dict:\n",
    "        present_dict[keyword] = {}\n",
    "    if emotion not in present_dict[keyword]:\n",
    "        present_dict[keyword][emotion] = 0\n",
    "    present_dict[keyword][emotion] += 1\n",
    "\n",
    "#Create a csv file with the format: id,label\n",
    "header=['Id','Label']\n",
    "emotion_ids = []\n",
    "keyword_ids = []\n",
    "rows = []\n",
    "i=1\n",
    "for emotion in present_df['emotion'].value_counts().index.tolist():\n",
    "    rows.append(str(i)+\",\"+emotion)\n",
    "    emotion_ids.append(emotion)\n",
    "    i+=1\n",
    "for keyword in present_dict:\n",
    "    rows.append(str(i)+\",\"+keyword)\n",
    "    keyword_ids.append(keyword)\n",
    "    i+=1\n",
    "\n",
    "#Save rows to csv file\n",
    "with open('sentiment_analysis/gephi/lilmiquela_present_emotion_nodes.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    f.write('\\n'.join(rows))\n",
    "\n",
    "#Create a csv file with the format: Source,Target,Label,Weight\n",
    "header=['Source','Target','Label','Weight']\n",
    "rows = []\n",
    "i=1\n",
    "for keyword,emotion in present_dict.items():\n",
    "    #Get emotion key from emotion value\n",
    "    emotion_key = list(emotion.keys())[0]\n",
    "    #Get emotion count as weight\n",
    "    weight = list(emotion.values())[0]\n",
    "    #Get emotion id\n",
    "    target = emotion_ids.index(emotion_key)+1\n",
    "    #Get keyword id\n",
    "    source = i+len(emotion_ids)\n",
    "    rows.append(str(source)+\",\"+str(target)+\",\"+keyword+\" - \"+emotion_key+\",\"+str(weight))\n",
    "    i+=1\n",
    "#Save rows to csv file\n",
    "with open('sentiment_analysis/gephi/lilmiquela_present_emotion_edges.csv', 'w') as f:\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    f.write('\\n'.join(rows))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past Emotions\n",
      "['others', 'surprise', 'joy', 'disgust', 'fear', 'sadness', 'anger']\n",
      "Present Emotions\n",
      "['others', 'joy', 'surprise', 'disgust', 'fear', 'anger', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "#Print each emotion as a list\n",
    "print('Past Emotions')\n",
    "print(past_df['emotion'].value_counts().index.tolist())\n",
    "print('Present Emotions')\n",
    "print(present_df['emotion'].value_counts().index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
